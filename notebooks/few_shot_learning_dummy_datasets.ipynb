{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning with Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview \n",
    "\n",
    "1) Short introduction on Foundation Models and Presto\n",
    "2) Definition of Few-Shot learning\n",
    "3) Apply Presto to perfrom Few-Shot learning on a regression and a classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Foundation Models\n",
    "\n",
    "A Foundation Model is a model trained on large and diverse unlabeled datasets to learn general patterns and features of the data. Thanks to its strong generalization capabilities, such a model can be adapted for a wide range of applications that use similar types of input data.\n",
    "\n",
    "**Presto** (**P**retrained **Re**mote **S**ensing **T**ransf**o**rmer) is a foundation model trained on a large, unlabeled dataset of Sentinel-2, Sentinel-1, Meteorological and Topography pixel-timeseries data. It is able to capture long-range relationships across time and sensor dimensions, improving the signal-to-noise ratio and providing a concise, informative representation of the inputs. \n",
    "In this project, We made use of the [Presto](https://github.com/WorldCereal/prometheo.git) version developed in collaboration with WorldCereal\n",
    "\n",
    "Originally trained on monthly composites, Presto has been refined to be able to ingest dekadal data and to be fine-tuned for regression and classification tasks.\n",
    "\n",
    "### 2) Few-Shot Learning\n",
    "\n",
    "Few-shot learning aims to develop models that can learn from a small number of labeled instances while enhancing generalization and performance on new, unseen examples.\n",
    "\n",
    "Given a dataset with only a few annotated examples, we can fine-tune a pretrained foundation model to either directly handle the downstream task or generate compressed representations of the inputs (embeddings), which can then be used to train a machine learning model for the downstream task. We hereby show-case the former scenario, whose overview is depicted in the figure below.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/ScaleAG_pipeline_overview_presto.jpg\" alt=\"Overview of a Foundation Model fine tuned for different downstream tasks and applications.\" width=\"700\" />\n",
    "    <p><em>Overview of a Foundation Model fine tuned for different downstream tasks and applications.</em></p>\n",
    "</div>\n",
    "\n",
    "### 3) Implementing Few-Shot learning with Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from scaleagdata_vito.presto.datasets_prometheo import ScaleAgDataset\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import generate_input_for_extractions, extract\n",
    "from scaleagdata_vito.presto.utils import evaluate_finetuned_model\n",
    "from scaleagdata_vito.presto.presto_df import load_dataset\n",
    "from scaleagdata_vito.presto.utils import train_test_val_split, finetune_on_task, load_finetuned_model, get_pretrained_model_url, get_resources_dir\n",
    "from scaleagdata_vito.presto.inference import PrestoPredictor, reshape_result, plot_results\n",
    "from scaleagdata_vito.utils.map import ui_map\n",
    "from scaleagdata_vito.utils.dateslider import date_slider\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import collect_inputs_for_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start...\n",
    "\n",
    "**Check your data!** Investigate validity of geometries uniqueness of sample IDs, presence of outliers and so on before starting the extraction. Achieving good performance making use of a limited amount of data is a challening task per se. Therefore, **the quality of your data will greatly impact your final results.**\n",
    "\n",
    "Data requirements:\n",
    "- Points or Polygons (will be aggregated in points)\n",
    "- Lat-Lon (crs:4326) \n",
    "- Format: parquet, GeoJSON, shapefile, GPKG\n",
    "For each geometry:\n",
    "- Date (if available) \n",
    "- Unique ID\n",
    "- Annotations\n",
    "\n",
    "Good practice:\n",
    "\n",
    "Remove polygons close to borders (e.g. apply buffer) to ensure data are contained in the field\n",
    "If the annotations are accurate, point geometries should be preferred. However, especially in regression tasks (i.e., continuous output values) such us yield estimation the target values might be noisy. In that case, we recommend subdividing the polygons in subfields of 20m x 20m (to cover more measurements) and computing the median yield for a smoother and more reliable target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements for running the extractions\n",
    "- Account in [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/). You can sign up for free and have a monthly availability of 10000 credits.\n",
    "- A dataset with valid geometries (Points or Polygons) in lat-lon projection.\n",
    "- Preferably a dataset with unique IDs per sample \n",
    "- A labelled dataset. Not required for the extraction process, but for the following fine-tuning steps.\n",
    "\n",
    "#### EO data extractions\n",
    "In this first step, we extract for each sample in your dataset the required EO time series from CDSE using OpenEO.\n",
    "For running the job, the user should indicate the following job_dictionary fields:\n",
    "\n",
    "```python\n",
    "    job_params = dict(\n",
    "        output_folder=..., # where to save the extracted dataset\n",
    "        input_df=..., # input georeferenced dataset to run the extractions for \n",
    "        start_date=..., # string indicating from which date to extract data  \n",
    "        end_date=..., # string indicating until which date to extract the data \n",
    "        unique_id_column=..., # name of the column in the input_df containing the unique ID of the samples  \n",
    "        composite_window=..., # \"month\" or \"dekad\" are supported. Default is \"dekad\"\n",
    "    )\n",
    "```\n",
    "in particular:\n",
    "- If the `date` information associated with the label is provided, the `start_date` of the time-series is automatically set to 9 months before the date, whereas the `end_date` is set to 9 months after. If `date` is not available, the user needs to manually indicate the desired `start_date` and `end_date` for the extractions.\n",
    "- `composite_window` indicates the time-series granularity, which can be dekadal or monthly. \n",
    "  - `dekad`: each time step in the extracted time series corresponds to a mean-compositing operation on 10-days acquisitions. Accordingly with the start and end date, each month will be covered by 3 time steps which, by default, correspond to the 1st, 11th and 21th of the month. \n",
    "  - `month`: each time step in the extracted time series corresponds to a mean-compositing operation on 30-days acquisitions. Each month will be covered by 1 time step which, by default, correspond to the 1st of the month.\n",
    "\n",
    "The following decadal/monthly time series will be extracted for the indicated time range:\n",
    "\n",
    "- Sentinel-2 L2A data (all bands)\n",
    "- Sentinel-1 VH and VV\n",
    "- Average air temperature and precipitation sum derived from AgERA5\n",
    "- Slope and elevation from Copernicus DEM\n",
    "\n",
    "Presto accepts 1D time-series. Therefore, if Polygons are provided for the extractions, the latter are spatially aggregated in points which will correspond to the centroid lat lon geolocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression task: potato yield estimation \n",
    "\n",
    "The data covers fields in Belgium during the growing season of 2022. Each field polygon was partitioned in subfields of 20m x 20m. The latter are partitioned into training, validation and test sets. \n",
    "\n",
    "**NOTE:** This is a very small dummy dataset with randomized yield values. No meaningful results are expected from using such data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path(\"/home/giorgia/Private/data/scaleag/demo/regression\")\n",
    "input_df = get_resources_dir() / \"dummy_yield.geojson\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "unique_id_column = \"fieldname\"\n",
    "composite_window = \"dekad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check input data structure \n",
    "gpd.read_file(input_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_params = dict(\n",
    "    output_folder=output_folder,\n",
    "    input_df=input_df,\n",
    "    unique_id_column=unique_id_column,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "extract(generate_input_for_extractions(job_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset will be extracted, it can be loaded with the `load_dataset` function by specifying the path where the `.parquet` files have been downloaded. Moreover, the following manipulations of the dataset are also possible:\n",
    "\n",
    "- `window_of_interest`: the user can specify a time window of interest out of the whole available time-series. `start_date` and `end_date` should be provided as strings in a list.\n",
    "- `use_valid_time`: the user might want to define the window of interest based on the `date` the label is associated with. If so, also `required_min_timesteps` should be provided\n",
    "- `buffer_window`: buffers the `start_date` and `end_date` by the number of time steps here specified  \n",
    "\n",
    "In the following cell, we load the extracted dataset for 1 year of data.\n",
    "\n",
    "**NOTE:** this code currently assumes that we are dealing with 1 year of data falling in the same time period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presto datasets initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\n",
    "    files_root_dir=output_folder,\n",
    "    window_of_interest=[start_date, end_date],\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step splits the data into train, test and val datasets. the split can be performed by uniform sampling or by group sampling. The former is usually more suitable for the binary and multiclass classification tasks, to ensure the data distribution is represented in all the 3 sets. The latter is more specific for cases where we want to avoid data autocorrelation and so data leakage between training and val/test sets.\n",
    "In the case of yield estimation, for instance, we often have samples coming from the same field. So we might want to separate the data based on the field they belong to to better test the model generalization capabilities.\n",
    "Therefore:\n",
    "- `uniform_sample_by`: pass the name of the column in the dataframe to perform uniform sampling on\n",
    "- `group_sample_by`: pass the name of the column in the dataframe to perform the group sampling on\n",
    "\n",
    "`sampling_fraction` indicates the proportion of the training set out of the whole dataset. the remaining percentage will be equally devided into validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = train_test_val_split(df=df, group_sample_by=\"parentname\", sampling_frac=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the parameters needed for initializing presto datasets for the specific task:\n",
    "- `num_timesteps`: can be inferred by the max number of the `available_timesteps` \n",
    "- `target_name`: name of the column containing the target data\n",
    "- `upper_bound` and `lower_bound`: these should be set to the min and max of the distribution. Therefore, it is important to get rid of potential outlaiers beforehand.\n",
    "\n",
    "**NOTE:** upper and lower bounds are also used to normalize the targets during the training process. Therefore it is important to keep track of such values to convert the predictions to the original units in the inference step!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution to check for outliers to exclude if needed\n",
    "num_timesteps = df.available_timesteps.max()\n",
    "task_type = \"regression\"\n",
    "target_name = \"median_yield\"\n",
    "upper_bound = df[target_name].max() \n",
    "lower_bound = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Initialize the training, validation and test datasets objects to be used for training Presto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize datasets\n",
    "train_ds = ScaleAgDataset(\n",
    "    df_train,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=upper_bound,\n",
    "    lower_bound=lower_bound,\n",
    ")\n",
    "val_ds = ScaleAgDataset(\n",
    "    df_val,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=upper_bound,\n",
    "    lower_bound=lower_bound,\n",
    ")\n",
    "test_ds = ScaleAgDataset(\n",
    "    df_test,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=upper_bound,\n",
    "    lower_bound=lower_bound,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presto Finetuning\n",
    "\n",
    "In this section Presto will be Fine-Tuned in a supervised way for the target downstream task. first we set up the following experiment parameters:\n",
    "\n",
    "- `output_dir` : where to dave the model \n",
    "- `experiment_name` : the model name\n",
    "- `pretrained_model_path` : pretrained presto model to start the fine tuning from. Can be a string indicating the path to the model or a url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set models hyperparameters\n",
    "model_output_dir = Path(\"/home/giorgia/Private/data/scaleag/demo/regression/\")\n",
    "experiment_name = \"presto-ss-wc-10D-ft-dek\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the model with finetuning head starting from the pretrained model\n",
    "finetuned_model = finetune_on_task(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    pretrained_model_path=get_pretrained_model_url(composite_window=composite_window),\n",
    "    output_dir=model_output_dir, \n",
    "    experiment_name=experiment_name,\n",
    "    num_workers=0,\n",
    "    )\n",
    "evaluate_finetuned_model(finetuned_model, test_ds, num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference using Fine-Tuned end-to-end Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we apply the fine tuned model to generate a yield map on an unseen area. \n",
    "We need to indicate the spatial and temporal extent. The 2 cells below, offer a simple way for the user to provide these information and perform once again the extraction from CDSE of the EO time-series required by Presto. \n",
    "We also need to indicate the `output_dir` of where to save the datacube of the extraction, its `output_filename` and the `composite_window` which will be the same as used for finetuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = ui_map(area_limit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 1 year of data\n",
    "slider = date_slider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"/home/giorgia/Private/data/scaleag/demo/regression\")\n",
    "output_filename = \"inference_area\"\n",
    "inference_file = output_dir / f\"{output_filename}.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_inputs_for_inference(\n",
    "    spatial_extent=map.get_extent(),\n",
    "    temporal_extent=slider.get_processing_period(),\n",
    "    output_path=output_dir,\n",
    "    output_filename=f\"{output_filename}.nc\",\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the datacube has been extracted, we can perform the inference task using the finetuned model and visualize the predicted map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_file = get_resources_dir() / \"inference_area_tevuren.nc\"\n",
    "mask_path = get_resources_dir() / \"LPIS_flanders_potatoes_2022.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = load_finetuned_model(model_output_dir / experiment_name, task_type=task_type)\n",
    "presto_model = PrestoPredictor(\n",
    "    model=finetuned_model,\n",
    "    batch_size=50,\n",
    "    task_type=task_type,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "predictions = presto_model.predict(inference_file, upper_bound=upper_bound, lower_bound=lower_bound, mask_path=mask_path)\n",
    "predictions_map = reshape_result(predictions, path_to_input_file=inference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(prob_map=predictions_map, path_to_input_file=inference_file, task=task_type, ts_index=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary task: crop/no-crop\n",
    "\n",
    "Now we test the few-shot learning on a binary task. We Fine-Tune presto on datapoints sampled from Flanders on 2021. This time, the dataset is the result of a monthly compositing. We initialize the parameters for the dataset preparation accordingly  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path(\"/home/giorgia/Private/data/scaleag/demo/worldcereal/\")\n",
    "input_df = get_resources_dir() / \"dummy_cropland.geojson\"\n",
    "task_type = \"binary\"\n",
    "target_name = \"LANDCOVER_LABEL\"\n",
    "composite_window = \"month\"\n",
    "unique_id_column = \"sample_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_params = dict(\n",
    "    output_folder=output_folder,\n",
    "    input_df=input_df,\n",
    "    unique_id_column=unique_id_column,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "extract(generate_input_for_extractions(job_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presto datasets initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\n",
    "    files_root_dir=output_folder,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "train_df, val_df, test_df = train_test_val_split(df=df, uniform_sample_by=target_name, sampling_frac=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a binary classification task, specifying the upper and lower bounds won't be necessary anymore. In case we are dealing with multiclass labels, we can convert the problem into a binary classification task by providing the `positive_labels` argument to the `ScaleAgDatset` class. The list of labels passed as value to `positive_labels` indicates which subset of classes should be interpreted as positive class (here \"crop\"). All the other labels will therefore be interpreted as negative class (here \"no-crop\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = df.available_timesteps.max()\n",
    "positive_labels = [10, 11, 12, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ScaleAgDataset(\n",
    "    train_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    positive_labels=positive_labels,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "val_ds = ScaleAgDataset(\n",
    "    val_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    positive_labels=positive_labels,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "test_ds = ScaleAgDataset(\n",
    "    test_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    positive_labels=positive_labels,\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presto Finetuning\n",
    "\n",
    "In this section Presto will be Fine-Tuned in a supervised way for the target downstream task. Once again, we set up the experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"presto_wc_ft_crop\"\n",
    "model_output_dir = Path(\"/home/giorgia/Private/data/scaleag/demo/worldcereal/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = finetune_on_task(\n",
    "                train_ds=train_ds,\n",
    "                val_ds=val_ds,\n",
    "                pretrained_model_path=get_pretrained_model_url(composite_window=composite_window),\n",
    "                output_dir=model_output_dir, \n",
    "                experiment_name=experiment_name,\n",
    "                num_workers=0,\n",
    "            )\n",
    "evaluate_finetuned_model(finetuned_model, test_ds, num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference using Fine-Tuned end-to-end Presto\n",
    "\n",
    "We now apply the finetuned model to an unseen area to perform the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = load_finetuned_model(model_output_dir / experiment_name, task_type=task_type)\n",
    "inference_file = get_resources_dir() / \"worldcereal_preprocessed_inputs.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto_model = PrestoPredictor(\n",
    "    model=finetuned_model,\n",
    "    batch_size=50,\n",
    "    task_type=task_type,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "predictions = presto_model.predict(inference_file)\n",
    "prob_map = reshape_result(predictions, path_to_input_file=inference_file)\n",
    "pred_map = presto_model.get_predictions(prob_map, threshold=0.75)\n",
    "plot_results(prob_map=prob_map, pred_map=pred_map, path_to_input_file=inference_file, task=task_type, ts_index=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
