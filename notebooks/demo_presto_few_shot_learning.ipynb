{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import catboost as cb\n",
    "import openeo\n",
    "from loguru import logger\n",
    "import geopandas as gpd\n",
    "import geojson\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from scaleagdata_vito.presto.datasets import ScaleAG10DDataset\n",
    "from scaleagdata_vito.presto.presto_df import (add_labels, xr_to_df, filter_ts)\n",
    "from scaleagdata_vito.presto.presto_utils import load_pretrained_model_from_url, get_encodings\n",
    "from scaleagdata_vito.openeo.preprocessing import scaleag_preprocessed_inputs_gfmap\n",
    "from openeo_gfmap import (\n",
    "    Backend,\n",
    "    BackendContext,\n",
    "    TemporalContext,\n",
    "    FetchType,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Presto pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-22 15:46:41.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.presto_utils\u001b[0m:\u001b[36mload_pretrained_model_from_url\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m Initialize Presto dekadal architecture with 10d ss trained WorldCereal Presto weights...\u001b[0m\n",
      "\u001b[32m2024-08-22 15:46:42.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.presto_utils\u001b[0m:\u001b[36mload_pretrained_model_from_url\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1m Initialize Presto dekadal architecture with 30d ss trained WorldCereal Presto weights...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Decadal and Monthly Presto models trained in self-supervised mode on WorldCereal data\n",
    "presto_ss_10d_wc = \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/scaleagdata/models/presto-ss-wc_10D.pt\"\n",
    "presto_ss_30d_wc = \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/scaleagdata/models/presto-ss-wc_30D.pt\"\n",
    "model_wc_10d = load_pretrained_model_from_url(\n",
    "    presto_ss_10d_wc, finetuned=False, ss_dekadal=True, strict=False, device=\"cpu\"\n",
    ")\n",
    "model_wc_30d = load_pretrained_model_from_url(\n",
    "    presto_ss_30d_wc, finetuned=False, ss_dekadal=False, strict=False, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot learning with Presto on yield task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    }
   ],
   "source": [
    "gdf = (\n",
    "    gpd.read_file(\n",
    "        \"/projects/TAP/HEScaleAgData/timeseries_modelling/datasets/apr2024_AVR_subfields/data/AVR_fields_10000_100000_subfields_yield_bel_nl_roads_removed.geojson\"\n",
    "    )\n",
    "    .iloc[:100]\n",
    "    .drop(columns=[\"date\"])\n",
    ")\n",
    "output_path = \"/home/vito/millig/gio/data/scaleag_demo/test\"\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set temporal range to generate product\n",
    "temporal_extent = TemporalContext(\n",
    "    start_date=\"2022-01-01\",\n",
    "    end_date=\"2022-12-31\",\n",
    ")\n",
    "\n",
    "connection = openeo.connect(\"https://openeo.creo.vito.be/openeo/\").authenticate_oidc()\n",
    "backend_context = BackendContext(Backend.CDSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected orbit direction: ASCENDING from max accumulated area overlap between bounds and products.\n"
     ]
    }
   ],
   "source": [
    "geometry_latlon = geojson.loads(gdf.to_json())\n",
    "inputs = scaleag_preprocessed_inputs_gfmap(\n",
    "    connection=connection,\n",
    "    backend_context=backend_context,\n",
    "    spatial_extent=geometry_latlon,\n",
    "    temporal_extent=temporal_extent,\n",
    "    fetch_type=FetchType.POINT,\n",
    "    disable_meteo=True,  # precompute meteo and upload in bucket\n",
    ")\n",
    "cube = inputs.aggregate_spatial(geometries=geometry_latlon, reducer=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 Job 'j-2408226c9d6442a9b9cecf54160428aa': send 'start'\n",
      "0:00:15 Job 'j-2408226c9d6442a9b9cecf54160428aa': created (progress 0%)\n",
      "0:00:32 Job 'j-2408226c9d6442a9b9cecf54160428aa': created (progress 0%)\n",
      "0:00:42 Job 'j-2408226c9d6442a9b9cecf54160428aa': created (progress 0%)\n",
      "0:00:57 Job 'j-2408226c9d6442a9b9cecf54160428aa': created (progress 0%)\n",
      "0:01:07 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:01:22 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:01:38 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:01:57 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:02:22 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:02:55 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:03:33 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:04:20 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:05:18 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:06:23 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:07:27 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:08:30 Job 'j-2408226c9d6442a9b9cecf54160428aa': running (progress N/A)\n",
      "0:09:30 Job 'j-2408226c9d6442a9b9cecf54160428aa': finished (progress 100%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/vito/millig/gio/data/scaleag_demo/test/timeseries.nc')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = cube.create_job(\n",
    "    outputfile=output_path,\n",
    "    out_format=\"NetCDF\",\n",
    "    title=\"Test_ScaleAgData\",\n",
    "    job_options={\n",
    "        \"driver-memory\": \"4g\",\n",
    "        \"executor-memoryOverhead\": \"4g\",\n",
    "        \"soft-error\": True,\n",
    "    },\n",
    "    # sample_by_feature=True,\n",
    ")\n",
    "job.start_and_wait()\n",
    "job.download_result(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = f\"{output_path}/timeseries.nc\"\n",
    "gdf_label_file = (\n",
    "    \"/projects/TAP/HEScaleAgData/\" \\\n",
    "    \"timeseries_modelling/datasets/apr2024_AVR_subfields/\" \\\n",
    "    \"data/AVR_fields_10000_100000_subfields_yield_bel_nl_roads_removed.geojson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3508: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "dataset = add_labels(xr_to_df(dataset_file), gdf_label_file)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: number of field IDs: 1, number of samples: 16\n",
      "Training: number of field IDs: 9, number of samples: 84\n"
     ]
    }
   ],
   "source": [
    "# split avoiding data leakage\n",
    "np.random.seed(3)\n",
    "df_p = dataset['parentname'].unique()\n",
    "train_frac = 0.90\n",
    "sample_idx = np.random.choice(range(0, len(df_p)), size=int(train_frac*len(df_p)), replace=False)\n",
    "train_df = dataset[dataset['parentname'].isin(df_p[sample_idx])].reset_index(drop=True)\n",
    "val_df = dataset[~dataset['parentname'].isin(df_p[sample_idx])].reset_index(drop=True)\n",
    "\n",
    "print(f\"Validation: number of field IDs: {len(val_df['parentname'].unique())}, number of samples: {len(val_df)}\")\n",
    "print(f\"Training: number of field IDs: {len(train_df['parentname'].unique())}, number of samples: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "target_name = \"median_yield\"\n",
    "train_ds = ScaleAG10DDataset(train_df, target_name=target_name, task=\"regression\")\n",
    "val_ds = ScaleAG10DDataset(val_df, target_name=target_name, task=\"regression\")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "dl_val = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/vito/millig/gio/scaleag-vito/src/scaleagdata_vito/presto/datasets.py\", line 200, in __getitem__\n    eo, mask_per_token, latlon, month, target = self.row_to_arrays(\n  File \"/home/vito/millig/gio/scaleag-vito/src/scaleagdata_vito/presto/datasets.py\", line 86, in row_to_arrays\n    [float(row_d[df_val.format(t)]) for t in range(cls.NUM_TIMESTEPS)]\n  File \"/home/vito/millig/gio/scaleag-vito/src/scaleagdata_vito/presto/datasets.py\", line 86, in <listcomp>\n    [float(row_d[df_val.format(t)]) for t in range(cls.NUM_TIMESTEPS)]\nKeyError: 'METEO-precipitation_flux-ts0-100m'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresto-ss-wc_10D\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m encodings_np, targets \u001b[38;5;241m=\u001b[39m \u001b[43mget_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_wc_10d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting Catboost model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m encodings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m cbm \u001b[38;5;241m=\u001b[39m cb\u001b[38;5;241m.\u001b[39mCatBoostRegressor(\n\u001b[1;32m      7\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      8\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     loss_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/gio/scaleag-vito/src/scaleagdata_vito/presto/presto_utils.py:108\u001b[0m, in \u001b[0;36mget_encodings\u001b[0;34m(dl, pretrained_presto)\u001b[0m\n\u001b[1;32m    106\u001b[0m pretrained_presto\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    107\u001b[0m batch_encodings, batch_targets \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, dw, latlons, month, variable_mask \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[1;32m    109\u001b[0m     batch_targets\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[1;32m    110\u001b[0m     x_f, dw_f, latlons_f, month_f, variable_mask_f \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    111\u001b[0m         t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m (x, dw, latlons, month, variable_mask)\n\u001b[1;32m    112\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/vito/millig/gio/scaleag-vito/src/scaleagdata_vito/presto/datasets.py\", line 200, in __getitem__\n    eo, mask_per_token, latlon, month, target = self.row_to_arrays(\n  File \"/home/vito/millig/gio/scaleag-vito/src/scaleagdata_vito/presto/datasets.py\", line 86, in row_to_arrays\n    [float(row_d[df_val.format(t)]) for t in range(cls.NUM_TIMESTEPS)]\n  File \"/home/vito/millig/gio/scaleag-vito/src/scaleagdata_vito/presto/datasets.py\", line 86, in <listcomp>\n    [float(row_d[df_val.format(t)]) for t in range(cls.NUM_TIMESTEPS)]\nKeyError: 'METEO-precipitation_flux-ts0-100m'\n"
     ]
    }
   ],
   "source": [
    "model_name = \"presto-ss-wc_10D\"\n",
    "encodings_np, targets = get_encodings(dl_train, model_wc_10d)\n",
    "\n",
    "logger.info(f\"Fitting Catboost model with {model_name} encodings\")\n",
    "\n",
    "cbm = cb.CatBoostRegressor(\n",
    "    random_state=3,\n",
    "    task_type=\"GPU\",\n",
    "    devices=\"0:1\",\n",
    "    logging_level=\"Silent\",\n",
    "    loss_function=\"RMSE\",\n",
    ")\n",
    "train_dataset = cb.Pool(encodings_np, targets)\n",
    "cbm.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shot learning with Presto on Crop/no-Crop task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from presto.dataset import WorldCerealLabelled10DDataset\n",
    "\n",
    "# Load WorldCereal dataset from artifactory\n",
    "\n",
    "wc_train_dataset = pd.read_parquet(\n",
    "    \"/home/vito/millig/gio/data/presto_ft/rawts-10d_train.parquet\"\n",
    ")\n",
    "\n",
    "wc_val_dataset = pd.read_parquet(\"/home/vito/millig/gio/data/presto_ft/rawts-10d_val.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3732 933\n"
     ]
    }
   ],
   "source": [
    "wc_train_dataset = wc_train_dataset.sample(frac=0.005)\n",
    "wc_val_dataset = wc_val_dataset.sample(frac=0.005)\n",
    "\n",
    "print(len(wc_train_dataset), len(wc_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vito/millig/miniconda3/envs/sadenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "wc_train_ds = WorldCerealLabelled10DDataset(wc_train_dataset)\n",
    "wc_val_ds = WorldCerealLabelled10DDataset(wc_val_dataset)\n",
    "wc_dl_train = DataLoader(\n",
    "    wc_train_ds,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "wc_dl_val = DataLoader(\n",
    "    wc_val_ds,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"presto-ss-wc_10D\"\n",
    "encodings_np, targets = get_encodings(wc_dl_train, model_wc_10d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-22 15:48:10.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mFitting Catboost model with presto-ss-wc_10D encodings\u001b[0m\n",
      "TBB Warning: The number of workers is currently limited to 1. The request for 95 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f523bfa1a50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(f\"Fitting Catboost model with {model_name} encodings\")\n",
    "\n",
    "cbm = cb.CatBoostClassifier(\n",
    "    random_state=3,\n",
    "    task_type=\"GPU\",\n",
    "    devices=\"0:1\",\n",
    "    logging_level=\"Silent\",\n",
    ")\n",
    "train_dataset = cb.Pool(encodings_np, targets)\n",
    "cbm.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sadenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
